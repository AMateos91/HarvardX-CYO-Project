---
title: "HarvardX Data Science Professional Certificate - CYO Capstone Project"
author: "Abraham Mateos"
date: "April 3, 2023"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Overview

This project is the solution for the “Choose Your Own” Project requirement, the second part of the Module 9 - Capstone course, inside the HarvardX Data Science Professional Certificate Program. The main target is to devise a credit card fraud detection system, based on a machine learning algorithm, which has been approached by several methods (one of them, the one shared below, is the main useful method).
Nowadays, we are absolutely exposed to possible fraud transactions in the Internet through e-commerce and online purchases. For this reason, several international authorities, such as the International Monetary Fund, have settled down for many years the principles of policy procedures to ensure and ease up the process of bank card fraud detection via online accounts’ machine learning code. In sum, this project may be a sample / resume of these proceeds which have been carried out on by the main world organizations. For this task, I have used all the techniques and resources learnt throughout all this program materials and courses.


# Introduction

The credit card fraud detection systems might be one of the main systems any banking entity should have established inside its own software structure. Under certain recent American and British universities research analysis, fraud is one of the major ethical issues in the credit card industry. The main aims are, firstly, to identify the different types of credit card fraud, and, in second place, to review alternative techniques and procedures that have been subject for fraud detection. 

The secondary target is to present, compare and analyze recently published discoveries in credit card fraud detection. This project defines common terms in credit card fraud and highlights key statistics and figures in this field. Depending on the type of fraud banks or credit card firms might face, several measures should be adopted and implemented. The proposals made in multiple documents are likely to have beneficial results and perks in terms of cost savings and time efficiency. The relevance of the application of the techniques reviewed here strikes on shrinking credit card fraud crimes volume. However, there are still ethical issues when genuine credit card clients are unclassified as fraudulent. 
 

# Aim of the project

The target in this project is to devise a machine learning algorithm approached by two ways or methods.
The approach to this problem is based on a specific pathway, since I came up with the linear regression model, but then opted for the decision tree method. Later, after deploying a logistic regression model, we implement a quite unique feature I hope will be welcome by the staff, the artificial neural network. This is a tool used generally to create the links among different features and variables straight forward into a machine learning training set, and quite easy to visualize.
And finally, the last step is an extreme gradient boosting machine learning regression model to train, under the Bernouilli distribution of fraud (1)/ not fraud (0), as in the previous approach. After getting this final model to work through iterations, we come up with plotting the AUC by using the own XGB model and both the test and the train dataset: indeed, we obtain slightly different results as you might appreciate in the end. 




# PROJECT

Data exploration, where we start out our way with a cleaning process through the functions learnt through this program courses:

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

library(dplyr)
library(tidyr)
library(tidyverse)
library(data.table)
library(stringr)
library(caTools)
library(rpart)
library(rpart.plot)
library(pROC)
library(gbm)
library(neuralnet) 
library(ranger)
library(car)
library(caret)
library(data.table)
dataset <- read.csv("C:/Users/Usuario/Desktop/creditcard.csv")

dim(dataset)
head(dataset, 5)
names(dataset)
var(dataset$Amount)
summary(dataset$Amount)
table(dataset$Class)
```


We start then a whole process of deleting NA values, for those ones which may be remaining inside our dataset or which, by default, may have been converted to:

```{r, echo = FALSE}
colSums(is.na(dataset))
dataset[is.na(dataset)] <- 0
head(dataset, 10)

```

Then we proceed with running the means of the main variables...

```{r, echo = FALSE}

mean(dataset$Amount, na.rm = TRUE)

mean(dataset$Class, na.rm = TRUE)

```

And then we summarize the resulting dataset after all this data cleaning process:

```{r Means, echo = FALSE}
df = dataset
summary(df)
head(df, 5)

```

And we plot it as a content review:

```{r Review Plot, echo = FALSE}
boxplot(df)
```

Winsorization is quite useful at this point. When an outlier is negatively impacting a model results, it is possible to replace this with a less extreme maximum value. In Winsorizing, values located out of a predetermined percentile range of the data are identified and set to this percentile. Rather, winsorizing a vector means a predefined quantum of the smallest and/or the largest values is replaced instead by less extreme values. Thus, the substitution values are the most extreme retained values in reference to those ones above 95th percentile. Data wrangling, using mainly head and scale functions which will let me scale or grow up the chosen amount to a more realistic sample to analyze:

```{r Winsorization, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

install.packages("robustHD", repos="https://cran.rstudio.com")
require(robustHD)
sum(df$Amount > quantile(df$Amount, .95))
df <- df %>% mutate(wins_total_amount = winsorize(Amount))
head(df, 5)

```

Data visualization:
We can plot by using histograms the distribution of transactions during time running:

```{r, echo=FALSE}
df %>%
ggplot(aes(x = Time, fill = factor(Class))) + 
geom_histogram(bins = 100) + 
labs(x = "Time elapsed since first transaction (seconds)", 
     y = "Number of transactions", 
     title = "Distribution of the transactions during time") +
facet_grid(Class ~ ., scales = 'free_y') + theme()
```

And analyze the correlation rate among the variables:

```{r, echo=FALSE}
correlation <- cor(df[, -1], method = "pearson")
corrplot::corrplot(correlation, number.cex = 1, 
                   method = "color", type = "full", 
                   tl.cex=0.7, tl.col="black")

```

Data wrangling: By using head and scale functions, I will be able to scale or grow up the selected amount
to a more realistic sample to be analyzed:

```{r, echo=FALSE}
df$Amount <- scale(df$Amount)
df_1 <- df[,-c(1)]
head(df_1)
```

We now proceed to model our dataset, by splitting it up in the partition of the train set and the test set:

```{r, echo=FALSE}
set.seed(123)
split <- sample.split(df_1$Class, SplitRatio=.70)
train <- df_1[split==TRUE, ]
test <- df_1[split==FALSE, ]
dim(train) 
dim(test)
```

We start our algorithms roadmap with a decision tree model, where as we have been taught through the program, we can achieve a partition of the credit card dataset, classify the type of the retrieved data and solve the linear regression, as we are managing continuous input and output data:

```{r Decision Tree, echo=FALSE}
dt_model <- rpart(Class~., df, method = 'class')
predicted <- predict(dt_model, df, type = 'class')
probability <- predict(dt_model, df, type = 'prob')
rpart.plot(dt_model)

```

I think it may be appropriate to implement now a logistic regression model, by making use of the class and test data, and the binomial distribution specification. With the concerning library for the ROC (Receiver Operating Characteristic), we then make the predictions and include them into our validation set (test set) and devise the respective visualization with the ‘roc’ function:

```{r Logistic Regression, echo=TRUE}

lr_model <- glm(Class~., train, family=binomial()) 

summary(lr_model)

predicted_2 <- predict(lr_model, test, probability = TRUE)

auc_curve <- roc(test$Class, predicted_2, plot= TRUE, col="green")

```


After the LR Model, for its own nature and reliance on the human nerves network and its similarity to this problem’s casuistry, it would be a good step to build up an Artificial Neural Network, where we should analyze our train set into a neural model, so that we can create a result which would fit the human mind. I have fulfilled this target by not reading the data as a linear description, but in a networking way, by linking and making sense among info nods with other data related to the same variable. Later, that training result is integrated into our test set, in order to come out with a result in a default case of 0.5 - 1:

```{r ANN, echo=FALSE}
nn_model <- neuralnet(Class~., train, linear.output=FALSE)
plot(nn_model)

predicted_3 <- compute(nn_model, test)
result_nn_model <- predicted_3$net.result
result_nn_model <- ifelse(result_nn_model>0.5, 1, 0)
```

As the last step, we take up an Extreme Gradient Boosting regression model (XGB model), which I think is the best step in order to come out with the best iteration among variables in the regression at the fitting process: 

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
set.seed(9560)
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
require(xgboost)

labels <- train$Class
```

We fit the model and configure its parameters:

```{r XGB, echo = FALSE}
xgb_fit <- xgboost(data = data.matrix(train[,-30]), 
                   label = labels,
                   eta = 0.1,
                   gamma = 0.1,
                   max_depth = 10, 
                   nrounds = 300, 
                   objective = "binary:logistic",
                   colsample_bytree = 0.6,
                   verbose = 0,
                   nthread = 7
)
xgb_pred <- predict(xgb_fit, data.matrix(test[,-30]))
```

We then calculate and plot the Area Under Curve for the ROC by using both the mostly used test dataset for this target and the train dataset as well, so that we can retrieve more realistic information on the decisions approach results:

```{r XGB AUC, echo = FALSE}
xgb_pred <- predict(xgb_fit, data.matrix(test[,-30]))
curve <- roc(test$Class, xgb_pred, plot = TRUE)
curve
plot(curve)

xgb_pred <- predict(xgb_fit, data.matrix(train[,-30]))
curve <- roc(train$Class, xgb_pred, plot = TRUE)
curve
plot(curve)
```

Conclusion: as we can appreciate through this approach, we have settled down some core and very well-known ML principles, but have arranged them in an order and manner (and as well along with some other intrinsic and previous tips) which have let me get that final AUC score after running the XGB model.

Nevertheless, needless to say it is quite essential to first-off implement the tree model method, so that we can then create an artificial neural network, and therefore, connect human mind features to credit cards data, gathering and collecting different data into each “variable classification”. To sum up, we carry out an extreme gradient boosting regression model, in order to apply different values to the different parameters included in our final training model, and then to figure out the best iteration.

